{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "x",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "columns",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "y",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "adc45734-259b-4048-8c52-0c3f6a43dc26",
       "rows": [
        [
         "0",
         "Write a Pandas program to Combine two DataFrame objects by filling null values in one DataFrame with non-null values from other DataFrame. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rdf1 = pd.DataFrame({'A': [None, 0, None], 'B': [3, 4, 5]})\rdf2 = pd.DataFrame({'A': [1, 1, 3], 'B': [3, None, 3]})\rdf1.combine_first(df2)\rprint(\"Original DataFrames:\")\rprint(df1)\rprint(\"--------------------\")\rprint(df2)\rprint(\"\\nMerge two dataframes with different columns:\")\rresult = df1.combine_first(df2)\rprint(result)\r"
        ],
        [
         "1",
         "Make a Pandas DataFrame with two-dimensional list | Python",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "# import pandas as pd \nimport pandas as pd  \n      \n# List1  \nlst = [['Geek', 25], ['is', 30], \n       ['for', 26], ['Geeksforgeeks', 22]] \n  \n# creating df object with columns specified    \ndf = pd.DataFrame(lst, columns =['Tag', 'number']) \nprint(df )"
        ],
        [
         "2",
         "Write a Pandas program to remove the html tags within the specified column of a given DataFrame. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rimport re as re\rdf = pd.DataFrame({\r    'company_code': ['Abcd','EFGF', 'zefsalf', 'sdfslew', 'zekfsdf'],\r    'date_of_sale': ['12/05/2002','16/02/1999','05/09/1998','12/02/2022','15/09/1997'],\r    'address': ['9910 Surrey <b>Avenue</b>','92 N. Bishop Avenue','9910 <br>Golden Star Avenue', '102 Dunbar <i></i>St.', '17 West Livingston Court']\r})\rprint(\"Original DataFrame:\")\rprint(df)\rdef remove_tags(string):\r    result = re.sub('<.*?>','',string)\r    return result\rdf['with_out_tags']=df['address'].apply(lambda cw : remove_tags(cw))\rprint(\"\\nSentences without tags':\")\rprint(df)\r"
        ],
        [
         "3",
         "Write a Pandas program to create a Pivot table and find survival rate by gender, age wise of various classes. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rimport numpy as np\rdf = pd.read_csv('titanic.csv')\rresult  =  df.pivot_table('survived', index=['sex','age'], columns='class')\rprint(result)\r"
        ],
        [
         "4",
         "Apply uppercase to a column in Pandas dataframe in Python",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "# Import pandas package \nimport pandas as pd \n    \n# making data frame \ndata = pd.read_csv(\"https://media.geeksforgeeks.org/wp-content/uploads/nba.csv\") \n    \n# calling head() method  \n# storing in new variable \ndata_top = data.head(10) \n    \n# display \ndata_top"
        ],
        [
         "5",
         "Create a dataframe of ten rows, four columns with random values. Write a Pandas program to highlight the maximum value in last two columns. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rimport numpy as np\rnp.random.seed(24)\rdf = pd.DataFrame({'A': np.linspace(1, 10, 10)})\rdf = pd.concat([df, pd.DataFrame(np.random.randn(10, 4), columns=list('BCDE'))],\r               axis=1)\rdf.iloc[0, 2] = np.nan\rdf.iloc[3, 3] = np.nan\rdf.iloc[4, 1] = np.nan\rdf.iloc[9, 4] = np.nan\rprint(\"Original array:\")\rprint(df)\rdef highlight_max(s):\r    '''\r    highlight the maximum in a Series green.\r    '''\r    is_max = s == s.max()\r    return ['background-color: green' if v else '' for v in is_max]\r\rprint(\"\\nHighlight the maximum value in last two columns:\")\rdf.style.apply(highlight_max,subset=pd.IndexSlice[:, ['D', 'E']])\r"
        ],
        [
         "6",
         "Write a Pandas program to check whether only proper case or title case is present in a given column of a DataFrame. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rdf = pd.DataFrame({\r    'company_code': ['Abcd','EFGF', 'Hhhh', 'abcd', 'EAWQaaa'],\r    'date_of_sale ': ['12/05/2002','16/02/1999','25/09/1998','12/02/2022','15/09/1997'],\r    'sale_amount': [12348.5, 233331.2, 22.5, 2566552.0, 23.0]})\r\rprint(\"Original DataFrame:\")\rprint(df)\rprint(\"\\nIs proper case or title case?\")\rdf['company_code_is_title'] = list(map(lambda x: x.istitle(), df['company_code']))\rprint(df)\r"
        ],
        [
         "7",
         "Write a Pandas program to convert a specified character column in upper/lower cases in a given DataFrame. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rdf = pd.DataFrame({\r    'company_code': ['Abcd','EFGF', 'zefsalf', 'sdfslew', 'zekfsdf'],\r    'date_of_sale': ['12/05/2002','16/02/1999','25/09/1998','12/02/2022','15/09/1997'],\r    'sale_amount': [12348.5, 233331.2, 22.5, 2566552.0, 23.0]\r})\r\rdf1 = pd.DataFrame({\r    'company_code': ['Abcd','EFGF', 'zefsalf', 'sdfslew', 'zekfsdf'],\r    'date_of_sale': ['12/05/2002','16/02/1999','25/09/1998','12/02/2022','15/09/1997'],\r    'sale_amount': [12348.5, 233331.2, 22.5, 2566552.0, 23.0]\r})\rprint(\"Original DataFrame:\")\rprint(df)\rprint(\"\\nUpper cases in comapny_code:\")\rdf['upper_company_code'] = list(map(lambda x: x.upper(), df['company_code']))\rprint(df)\rprint(\"\\nLower cases in comapny_code:\")\rdf1['lower_company_code'] = list(map(lambda x: x.lower(), df1['company_code']))\rprint(df1)\r"
        ],
        [
         "8",
         "Write a Pandas program to create a Pivot table and find manager wise, salesman wise total sale and also display the sum of all sale amount at the bottom. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rimport numpy as np\rdf = pd.read_excel('E:\\SaleData.xlsx')\rtable = pd.pivot_table(df,index=[\"Manager\",\"SalesMan\"],values=[\"Units\",\"Sale_amt\"],\r               aggfunc=[np.sum],fill_value=0,margins=True)\rprint(table)\r"
        ],
        [
         "9",
         "Make a Pandas DataFrame with two-dimensional list | Python",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "# import pandas as pd \nimport pandas as pd  \n      \n# List1  \nlst = [['Geek', 25], ['is', 30], \n       ['for', 26], ['Geeksforgeeks', 22]] \n  \n# creating df object with columns specified    \ndf = pd.DataFrame(lst, columns =['Tag', 'number']) \nprint(df )"
        ],
        [
         "10",
         "Write a Pandas program to check whether only numeric values present in a given column of a DataFrame.",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rdf = pd.DataFrame({\r    'company_code': ['Company','Company a001', '2055', 'abcd', '123345'],\r    'date_of_sale ': ['12/05/2002','16/02/1999','25/09/1998','12/02/2022','15/09/1997'],\r    'sale_amount': [12348.5, 233331.2, 22.5, 2566552.0, 23.0]})\r\t\rprint(\"Original DataFrame:\")\rprint(df)\rprint(\"\\nNumeric values present in company_code column:\")\rdf['company_code_is_digit'] = list(map(lambda x: x.isdigit(), df['company_code']))\rprint(df)\r\r"
        ],
        [
         "11",
         "Write a Pandas program to split a dataset, group by one column and get mean, min, and max values by group, also change the column name of the aggregated metric. Using the following dataset find the mean, min, and max values of purchase amount (purch_amt) group by customer id (customer_id). ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rpd.set_option('display.max_rows', None)\r#pd.set_option('display.max_columns', None)\rdf = pd.DataFrame({\r    'school_code': ['s001','s002','s003','s001','s002','s004'],\r    'class': ['V', 'V', 'VI', 'VI', 'V', 'VI'],\r    'name': ['Alberto Franco','Gino Mcneill','Ryan Parkes', 'Eesha Hinton', 'Gino Mcneill', 'David Parkes'],\r    'date_Of_Birth ': ['15/05/2002','17/05/2002','16/02/1999','25/09/1998','11/05/2002','15/09/1997'],\r    'age': [12, 12, 13, 13, 14, 12],\r    'height': [173, 192, 186, 167, 151, 159],\r    'weight': [35, 32, 33, 30, 31, 32],\r    'address': ['street1', 'street2', 'street3', 'street1', 'street2', 'street4']},\r    index=['S1', 'S2', 'S3', 'S4', 'S5', 'S6'])\rprint(\"Original DataFrame:\")\rprint(df)\rprint('\\nChange the name of an aggregated metric:')\rgrouped_single = df.groupby('school_code').agg({'age': [(\"mean_age\",\"mean\"), (\"min_age\", \"min\"), (\"max_age\",\"max\")]})\rprint(grouped_single)\r"
        ],
        [
         "12",
         "Create a dataframe of ten rows, four columns with random values. Write a Pandas program to display the dataframe in Heatmap style. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rimport numpy as np\rimport seaborn as sns\r\rnp.random.seed(24)\rdf = pd.DataFrame({'A': np.linspace(1, 10, 10)})\rdf = pd.concat([df, pd.DataFrame(np.random.randn(10, 4), columns=list('BCDE'))],\r               axis=1)\rprint(\"Original array:\")\rprint(df)\rprint(\"\\nDataframe - Heatmap style:\")\r\rcm = sns.light_palette(\"red\", as_cmap=True)\r \rdf.style.background_gradient(cmap='viridis') \r"
        ],
        [
         "13",
         "Visualize data from CSV file in Python",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import matplotlib.pyplot as plt\nimport csv\n  \nx = []\ny = []\n  \nwith open('biostats.csv','r') as csvfile:\n    plots = csv.reader(csvfile, delimiter = ',')\n      \n    for row in plots:\n        x.append(row[0])\n        y.append(int(row[2]))\n  \nplt.bar(x, y, color = 'g', width = 0.72, label = \"Age\")\nplt.xlabel('Names')\nplt.ylabel('Ages')\nplt.title('Ages of different persons')\nplt.legend()\nplt.show()"
        ],
        [
         "14",
         "Write a Pandas program to extract elements in the given positional indices along an axis of a dataframe. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd \rimport numpy as np\rsales_arrays = [['sale1', 'sale1', 'sale3', 'sale3', 'sale2', 'sale2', 'sale4', 'sale4'],\r          ['city1', 'city2', 'city1', 'city2', 'city1', 'city2', 'city1', 'city2']]\rsales_tuples = list(zip(*sales_arrays))\rsales_index = pd.MultiIndex.from_tuples(sales_tuples, names=['sale', 'city'])\rprint(\"\\nConstruct a Dataframe using the said MultiIndex levels:\")\rdf = pd.DataFrame(np.random.randn(8, 5), index=sales_index)\rprint(df)\rprint(\"\\nSelect 1st, 2nd and 3rd row of the said DataFrame:\")\rpositions = [1, 2, 5]\rprint(df.take([1, 2, 5]))\r\rprint(\"\\nTake elements at indices 1 and 2 along the axis 1 (column selection):\")\rprint(df.take([1, 2], axis=1))\r\rprint(\"\\nTake elements at indices 4 and 3 using negative integers along the axis 1 (column selection):\")\rprint(df.take([-1, -2], axis=1))\r"
        ],
        [
         "15",
         "Write a Pandas program convert the first and last character of each word to upper case in each word of a given series. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rseries1 = pd.Series(['php', 'python', 'java', 'c#'])\rprint(\"Original Series:\")\rprint(series1)\rresult = series1.map(lambda x: x[0].upper() + x[1:-1] + x[-1].upper())\rprint(\"\\nFirst and last character of each word to upper case:\")\rprint(result)\r"
        ],
        [
         "16",
         "Select row with maximum and minimum value in Pandas dataframe in Python",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "# importing pandas and numpy\nimport pandas as pd\nimport numpy as np\n  \n# data of 2018 drivers world championship\ndict1 ={'Driver':['Hamilton', 'Vettel', 'Raikkonen',\n                  'Verstappen', 'Bottas', 'Ricciardo',\n                  'Hulkenberg', 'Perez', 'Magnussen', \n                  'Sainz', 'Alonso', 'Ocon', 'Leclerc',\n                  'Grosjean', 'Gasly', 'Vandoorne',\n                  'Ericsson', 'Stroll', 'Hartley', 'Sirotkin'],\n                    \n        'Points':[408, 320, 251, 249, 247, 170, 69, 62, 56,\n                   53, 50, 49, 39, 37, 29, 12, 9, 6, 4, 1],\n                     \n        'Age':[33, 31, 39, 21, 29, 29, 31, 28, 26, 24, 37,\n                      22, 21, 32, 22, 26, 28, 20, 29, 23]}\n                        \n# creating dataframe using DataFrame constructor\ndf = pd.DataFrame(dict1)\nprint(df.head(10))"
        ],
        [
         "17",
         "Write a Pandas program to replace NaNs with a single constant value in specified columns in a DataFrame. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rimport numpy as np\rpd.set_option('display.max_rows', None)\r#pd.set_option('display.max_columns', None)\rdf = pd.DataFrame({\r'ord_no':[70001,np.nan,70002,70004,np.nan,70005,np.nan,70010,70003,70012,np.nan,70013],\r'purch_amt':[150.5,270.65,65.26,110.5,948.5,2400.6,5760,1983.43,2480.4,250.45, 75.29,3045.6],\r'ord_date': ['2012-10-05','2012-09-10',np.nan,'2012-08-17','2012-09-10','2012-07-27','2012-09-10','2012-10-10','2012-10-10','2012-06-27','2012-08-17','2012-04-25'],\r'customer_id':[3002,3001,3001,3003,3002,3001,3001,3004,3003,3002,3001,3001],\r'salesman_id':[5002,5003,5001,np.nan,5002,5001,5001,np.nan,5003,5002,5003,np.nan]})\rprint(\"Original Orders DataFrame:\")\rprint(df)\rprint(\"\\nReplace NaNs with a single constant value:\")\rresult = df['ord_no'].fillna(0, inplace=False)\rprint(result)\r"
        ],
        [
         "18",
         "Limited rows selection with given column in Pandas | Python",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "# Import pandas package \nimport pandas as pd \n    \n# Define a dictionary containing employee data \ndata = {'Name':['Jai', 'Princi', 'Gaurav', 'Anuj'], \n        'Age':[27, 24, 22, 32], \n        'Address':['Delhi', 'Kanpur', 'Allahabad', 'Kannauj'], \n        'Qualification':['Msc', 'MA', 'MCA', 'Phd']} \n    \n# Convert the dictionary into DataFrame  \ndf = pd.DataFrame(data) \n    \n# select three rows and two columns \nprint(df.loc[1:3, ['Name', 'Qualification']])"
        ],
        [
         "19",
         "Write a Pandas program to create a Pivot table and find manager wise, salesman wise total sale and also display the sum of all sale amount at the bottom. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rimport numpy as np\rdf = pd.read_excel('E:\\SaleData.xlsx')\rtable = pd.pivot_table(df,index=[\"Manager\",\"SalesMan\"],values=[\"Units\",\"Sale_amt\"],\r               aggfunc=[np.sum],fill_value=0,margins=True)\rprint(table)\r"
        ],
        [
         "20",
         "Write a Pandas program to replace more than one value with other values in a given DataFrame. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rdf = pd.DataFrame({\r    'company_code': ['A','B', 'C', 'D', 'A'],\r    'date_of_sale': ['12/05/2002','16/02/1999','25/09/1998','12/02/2022','15/09/1997'],\r    'sale_amount': [12348.5, 233331.2, 22.5, 2566552.0, 23.0]\r})\r\rprint(\"Original DataFrame:\")\rprint(df)\r\rprint(\"\\nReplace A with c:\")\rdf = df.replace([\"A\", \"D\"], [\"X\", \"Y\"])\rprint(df)\r"
        ],
        [
         "21",
         "Write a Pandas program to create a histogram to visualize daily return distribution of Alphabet Inc. stock price between two specific dates. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rimport matplotlib.pyplot as plt\rimport seaborn as sns\rdf = pd.read_csv(\"alphabet_stock_data.csv\")\rstart_date = pd.to_datetime('2020-4-1')\rend_date = pd.to_datetime('2020-9-30')                         \rdf['Date'] = pd.to_datetime(df['Date']) \rnew_df = (df['Date']>= start_date) & (df['Date']<= end_date)\rdf1 = df.loc[new_df]\rdf2 = df1[['Date', 'Adj Close']]\rdf3 = df2.set_index('Date')\rdaily_changes = df3.pct_change(periods=1)\rsns.distplot(daily_changes['Adj Close'].dropna(),bins=100,color='purple')\rplt.suptitle('Daily % return of Alphabet Inc. stock price,\\n01-04-2020 to 30-09-2020', fontsize=12, color='black')\rplt.grid(True)\rplt.show()\r"
        ],
        [
         "22",
         "Write a Pandas program to find the indexes of rows of a specified value of a given column in a DataFrame. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rdf = pd.DataFrame({\r    'school_code': ['s001','s002','s003','s001','s002','s004'],\r    'class': ['V', 'V', 'VI', 'VI', 'V', 'VI'],\r    'name': ['Alberto Franco','Gino Mcneill','Ryan Parkes', 'Eesha Hinton', 'Gino Mcneill', 'David Parkes'],\r    'date_of_birth': ['15/05/2002','17/05/2002','16/02/1999','25/09/1998','11/05/2002','15/09/1997'],\r    'weight': [35, 32, 33, 30, 31, 32]},\r     index =  [1, 2, 3, 4, 5, 6])\rprint(\"Original DataFrame with single index:\")\rprint(df)\rprint(\"\\nIndex of rows where specified column matches certain value:\")\rprint(df.index[df['school_code']=='s001'].tolist())\r"
        ],
        [
         "23",
         "Create a dataframe of ten rows, four columns with random values. Write a Pandas program to set dataframe background Color black and font color yellow. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rimport numpy as np\rnp.random.seed(24)\rdf = pd.DataFrame({'A': np.linspace(1, 10, 10)})\rdf = pd.concat([df, pd.DataFrame(np.random.randn(10, 4), columns=list('BCDE'))],\r               axis=1)\rdf.iloc[0, 2] = np.nan\rdf.iloc[3, 3] = np.nan\rdf.iloc[4, 1] = np.nan\rdf.iloc[9, 4] = np.nan\rprint(\"Original array:\")\rprint(df)\rprint(\"\\nBackground:black - fontcolor:yelow\")\rdf.style.set_properties(**{'background-color': 'black',\r                           'color': 'yellow'})\r"
        ],
        [
         "24",
         "Write a Pandas program to split the following given dataframe into groups based on school code and call a specific group with the name of the group. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rpd.set_option('display.max_rows', None)\r#pd.set_option('display.max_columns', None)\rstudent_data = pd.DataFrame({\r    'school_code': ['s001','s002','s003','s001','s002','s004'],\r    'class': ['V', 'V', 'VI', 'VI', 'V', 'VI'],\r    'name': ['Alberto Franco','Gino Mcneill','Ryan Parkes', 'Eesha Hinton', 'Gino Mcneill', 'David Parkes'],\r    'date_Of_Birth ': ['15/05/2002','17/05/2002','16/02/1999','25/09/1998','11/05/2002','15/09/1997'],\r    'age': [12, 12, 13, 13, 14, 12],\r    'height': [173, 192, 186, 167, 151, 159],\r    'weight': [35, 32, 33, 30, 31, 32],\r    'address': ['street1', 'street2', 'street3', 'street1', 'street2', 'street4']},\r    index=['S1', 'S2', 'S3', 'S4', 'S5', 'S6'])\r\rprint(\"Original DataFrame:\")\rprint(student_data)\rprint('\\nSplit the said data on school_code wise:')\rgrouped = student_data.groupby(['school_code'])\rprint(\"Call school code 's001':\")\rprint(grouped.get_group('s001'))\rprint(\"\\nCall school code 's004':\")\rprint(grouped.get_group('s004'))\r"
        ],
        [
         "25",
         "Drop rows from the dataframe based on certain condition applied on a column in Python",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "# importing pandas as pd\nimport pandas as pd\n  \n# Read the csv file and construct the \n# dataframe\ndf = pd.read_csv('nba.csv')\n  \n# Visualize the dataframe\nprint(df.head(15)\n  \n# Print the shape of the dataframe\nprint(df.shape)"
        ],
        [
         "26",
         "Write a Pandas program to check whether alphabetic values present in a given column of a DataFrame. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rdf = pd.DataFrame({\r    'company_code': ['Company','Company a001','Company 123', 'abcd', 'Company 12'],\r    'date_of_sale ': ['12/05/2002','16/02/1999','25/09/1998','12/02/2022','15/09/1997'],\r    'sale_amount': [12348.5, 233331.2, 22.5, 2566552.0, 23.0]})\r\rprint(\"Original DataFrame:\")\rprint(df)\rprint(\"\\nWhether Alphabetic values present in company_code column?\")\rdf['company_code_is_alpha'] = list(map(lambda x: x.isalpha(), df['company_code']))\rprint(df)\r"
        ],
        [
         "27",
         "Write a Pandas program to create a plot of adjusted closing prices, thirty days and forty days simple moving average of Alphabet Inc. between two specific dates. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rimport matplotlib.pyplot as plt\rdf = pd.read_csv(\"alphabet_stock_data.csv\")\rstart_date = pd.to_datetime('2020-4-1')\rend_date = pd.to_datetime('2020-9-30')                         \rdf['Date'] = pd.to_datetime(df['Date']) \rnew_df = (df['Date']>= start_date) & (df['Date']<= end_date)\rdf1 = df.loc[new_df]\rstock_data = df1.set_index('Date')\rclose_px = stock_data['Adj Close']\rstock_data['SMA_30_days'] = stock_data.iloc[:,4].rolling(window=30).mean() \rstock_data['SMA_40_days'] = stock_data.iloc[:,4].rolling(window=40).mean()\rplt.figure(figsize=[10,8])\rplt.grid(True)\rplt.title('Historical stock prices of Alphabet Inc. [01-04-2020 to 30-09-2020]\\n',fontsize=18, color='black')\rplt.plot(stock_data['Adj Close'],label='Adjusted Closing Price', color='black')\rplt.plot(stock_data['SMA_30_days'],label='30 days simple moving average', color='red')\rplt.plot(stock_data['SMA_40_days'],label='40 days simple moving average', color='green')\rplt.legend(loc=2)\rplt.show()\r"
        ],
        [
         "28",
         "Write a Pandas program to create a new DataFrame based on existing series, using specified argument and override the existing columns names. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rs1 = pd.Series([0, 1, 2, 3], name='col1')\rs2 = pd.Series([0, 1, 2, 3])\rs3 = pd.Series([0, 1, 4, 5], name='col3')\rdf = pd.concat([s1, s2, s3], axis=1, keys=['column1', 'column2', 'column3'])\rprint(df)\r"
        ],
        [
         "29",
         "Write a Pandas program to construct a series using the MultiIndex levels as the column and index. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd \rimport numpy as np\rsales_arrays = [['sale1', 'sale1', 'sale2', 'sale2', 'sale3', 'sale3', 'sale4', 'sale4'],\r          ['city1', 'city2', 'city1', 'city2', 'city1', 'city2', 'city1', 'city2']]\rsales_tuples = list(zip(*sales_arrays))\rprint(\"Create a MultiIndex:\")\rsales_index = pd.MultiIndex.from_tuples(sales_tuples, names=['sale', 'city'])\rprint(sales_tuples)\rprint(\"\\nConstruct a series using the said MultiIndex levels: \")\rs = pd.Series(np.random.randn(8), index = sales_index)\rprint(s)\r"
        ],
        [
         "30",
         "Create a dataframe of ten rows, four columns with random values. Write a Pandas program to make a gradient color on all the values of the said dataframe. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rimport numpy as np\rimport seaborn as sns\r\rnp.random.seed(24)\rdf = pd.DataFrame({'A': np.linspace(1, 10, 10)})\rdf = pd.concat([df, pd.DataFrame(np.random.randn(10, 4), columns=list('BCDE'))],\r               axis=1)\rprint(\"Original array:\")\rprint(df)\rprint(\"\\nDataframe - Gradient color:\")\rdf.style.background_gradient()\r"
        ],
        [
         "31",
         "Write a Pandas program to import excel data (coalpublic2013.xlsx ) into a dataframe and draw a bar plot where each bar will represent one of the top 10 production. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rimport numpy as np\rimport matplotlib.pyplot as plt\rdf = pd.read_excel('E:\\coalpublic2013.xlsx')\rsorted_by_production = df.sort_values(['Production'], ascending=False).head(10)\rsorted_by_production['Production'].head(10).plot(kind=\"barh\")\rplt.show()\r"
        ],
        [
         "32",
         "Create a dataframe of ten rows, four columns with random values. Write a Pandas program to highlight the maximum value in last two columns. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rimport numpy as np\rnp.random.seed(24)\rdf = pd.DataFrame({'A': np.linspace(1, 10, 10)})\rdf = pd.concat([df, pd.DataFrame(np.random.randn(10, 4), columns=list('BCDE'))],\r               axis=1)\rdf.iloc[0, 2] = np.nan\rdf.iloc[3, 3] = np.nan\rdf.iloc[4, 1] = np.nan\rdf.iloc[9, 4] = np.nan\rprint(\"Original array:\")\rprint(df)\rdef highlight_max(s):\r    '''\r    highlight the maximum in a Series green.\r    '''\r    is_max = s == s.max()\r    return ['background-color: green' if v else '' for v in is_max]\r\rprint(\"\\nHighlight the maximum value in last two columns:\")\rdf.style.apply(highlight_max,subset=pd.IndexSlice[:, ['D', 'E']])\r"
        ],
        [
         "33",
         "Write a Pandas program to rename names of columns and specific labels of the Main Index of the MultiIndex dataframe. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd \rimport numpy as np\rsales_arrays = [['sale1', 'sale1', 'sale2', 'sale2', 'sale3', 'sale3', 'sale4', 'sale4'],\r          ['city1', 'city2', 'city1', 'city2', 'city1', 'city2', 'city1', 'city2']]\rsales_tuples = list(zip(*sales_arrays))\rsales_index = pd.MultiIndex.from_tuples(sales_tuples, names=['sale', 'city'])\rprint(sales_tuples)\rprint(\"\\nConstruct a Dataframe using the said MultiIndex levels: \")\rdf = pd.DataFrame(np.random.randn(8, 5), index=sales_index)\rprint(df)\rprint(\"\\nRename the columns name of the said dataframe\")\rdf1 = df.rename(columns={0: \"col1\", 1: \"col2\", 2:\"col3\", 3:\"col4\", 4:\"col5\"})\rprint(df1)\rprint(\"\\nRename specific labels of the main index of the DataFrame\")\rdf2 = df1.rename(index={\"sale2\": \"S2\", \"city2\": \"C2\"})\rprint(df2)\r"
        ],
        [
         "34",
         "Different ways to iterate over rows in Pandas Dataframe in Python",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "# import pandas package as pd\nimport pandas as pd\n  \n# Define a dictionary containing students data\ndata = {'Name': ['Ankit', 'Amit', 'Aishwarya', 'Priyanka'],\n                'Age': [21, 19, 20, 18],\n                'Stream': ['Math', 'Commerce', 'Arts', 'Biology'],\n                'Percentage': [88, 92, 95, 70]}\n  \n# Convert the dictionary into DataFrame\ndf = pd.DataFrame(data, columns = ['Name', 'Age', 'Stream', 'Percentage'])\n  \nprint(\"Given Dataframe :\\n\", df)\n  \nprint(\"\\nIterating over rows using index attribute :\\n\")\n  \n# iterate through each row and select \n# 'Name' and 'Stream' column respectively.\nfor ind in df.index:\n     print(df['Name'][ind], df['Stream'][ind])"
        ],
        [
         "35",
         "Write a Pandas program to split a dataset to group by two columns and count by each row. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rpd.set_option('display.max_rows', None)\r#pd.set_option('display.max_columns', None)\rorders_data = pd.DataFrame({\r'ord_no':[70001,70009,70002,70004,70007,70005,70008,70010,70003,70012,70011,70013],\r'purch_amt':[150.5,270.65,65.26,110.5,948.5,2400.6,5760,1983.43,2480.4,250.45, 75.29,3045.6],\r'ord_date': ['2012-10-05','2012-09-10','2012-10-05','2012-08-17','2012-09-10','2012-07-27','2012-09-10','2012-10-10','2012-10-10','2012-06-27','2012-08-17','2012-04-25'],\r'customer_id':[3005,3001,3002,3009,3005,3007,3002,3004,3009,3008,3003,3002],\r'salesman_id': [5002,5005,5001,5003,5002,5001,5001,5006,5003,5002,5007,5001]})\rprint(\"Original Orders DataFrame:\")\rprint(orders_data)\rprint(\"\\nGroup by two columns and count by each row:\")\rresult = orders_data.groupby(['salesman_id','customer_id']).size().reset_index().groupby(['salesman_id','customer_id'])[[0]].max()\rprint(result)\r"
        ],
        [
         "36",
         "Write a Pandas program to create a graphical analysis of UFO (unidentified flying object) sighted by month. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rimport matplotlib.pyplot as plt\rimport seaborn as sns\rdf = pd.read_csv(r'ufo.csv')\rdf['Date_time'] = df['Date_time'].astype('datetime64[ns]')\rdf[\"ufo_yr\"] = df.Date_time.dt.month\rmonths_data = df.ufo_yr.value_counts()\rmonths_index = months_data.index  # x ticks\rmonths_values = months_data.get_values()\rplt.figure(figsize=(15,8))\rplt.xticks(rotation = 60)\rplt.title('UFO sighted by Month')\rplt.xlabel(\"Months\")\rplt.ylabel(\"Number of sighting\")\rmonths_plot = sns.barplot(x=months_index[:60],y=months_values[:60], palette = \"Oranges\")\r"
        ],
        [
         "37",
         "Write a Pandas program to set value in a specific cell in a given dataframe using index. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rdf = pd.DataFrame({\r    'school_code': ['s001','s002','s003','s001','s002','s004'],\r    'class': ['V', 'V', 'VI', 'VI', 'V', 'VI'],\r    'name': ['Alberto Franco','Gino Mcneill','Ryan Parkes', 'Eesha Hinton', 'Gino Mcneill', 'David Parkes'],\r    'date_of_birth': ['15/05/2002','17/05/2002','16/02/1999','25/09/1998','11/05/2002','15/09/1997'],\r    'weight': [35, 32, 33, 30, 31, 32]},\r     index = ['t1', 't2', 't3', 't4', 't5', 't6'])\rprint(\"Original DataFrame:\")\rprint(df)\rprint(\"\\nSet school code 's004' to 's005':\")\rdf.at['t6', 'school_code'] = 's005'\rprint(df)\rprint(\"\\nSet date_of_birth of 'Alberto Franco' to '16/05/2002':\")\rdf.at['t1', 'date_of_birth'] = '16/05/2002'\rprint(df)\r"
        ],
        [
         "38",
         "Formatting float column of Dataframe in Pandas in Python",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "# import pandas lib as pd\nimport pandas as pd\n  \n# create the data dictionary\ndata = {'Month' : ['January', 'February', 'March', 'April'],\n     'Expense': [ 21525220.653, 31125840.875, 23135428.768, 56245263.942]}\n  \n# create the dataframe\ndataframe = pd.DataFrame(data, columns = ['Month', 'Expense'])\n  \nprint(\"Given Dataframe :\\n\", dataframe)\n  \n# round to two decimal places in python pandas\npd.options.display.float_format = '{:.2f}'.format\n  \nprint('\\nResult :\\n', dataframe)"
        ],
        [
         "39",
         "Create a dataframe of ten rows, four columns with random values. Write a Pandas program to highlight dataframe's specific columns with different colors. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rimport numpy as np\rnp.random.seed(24)\rdf = pd.DataFrame({'A': np.linspace(1, 10, 10)})\rdf = pd.concat([df, pd.DataFrame(np.random.randn(10, 4), columns=list('BCDE'))],\r               axis=1)\rdf.iloc[0, 2] = np.nan\rdf.iloc[3, 3] = np.nan\rdf.iloc[4, 1] = np.nan\rdf.iloc[9, 4] = np.nan\rprint(\"Original array:\")\rprint(df)\rprint(\"\\nDifferent background color:\")\rcoldict = {'B':'red', 'D':'yellow'}\r\rdef highlight_cols(x):\r    #copy df to new - original data are not changed\r    df = x.copy()\r    #select all values to default value - red color\r    df.loc[:,:] = 'background-color: red'\r    #overwrite values grey color\r    df[['B','C', 'E']] = 'background-color: grey'\r    #return color df\r    return df    \r\rdf.style.apply(highlight_cols, axis=None)\r"
        ],
        [
         "40",
         "Write a Pandas program to create a line plot of the historical stock prices of Alphabet Inc. between two specific dates. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rimport matplotlib.pyplot as plt\rdf = pd.read_csv(\"alphabet_stock_data.csv\")\rstart_date = pd.to_datetime('2020-4-1')\rend_date = pd.to_datetime('2020-09-30')                         \rdf['Date'] = pd.to_datetime(df['Date']) \rnew_df = (df['Date']>= start_date) & (df['Date']<= end_date)\rdf1 = df.loc[new_df]\rdf2 = df1.set_index('Date')\rplt.figure(figsize=(5,5))\rplt.suptitle('Stock prices of Alphabet Inc.,\\n01-04-2020 to 30-09-2020', \\\r                 fontsize=18, color='black')\rplt.xlabel(\"Date\",fontsize=16, color='black')\rplt.ylabel(\"$ price\", fontsize=16, color='black')\r \rdf2['Close'].plot(color='green');\rplt.show()\r"
        ],
        [
         "41",
         "How to create multiple CSV files from existing CSV file using Pandas  in Python",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\n\n\n# initialise data dictionary.\ndata_dict = {'CustomerID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n              \n             'Gender': [\"Male\", \"Female\", \"Female\", \"Male\",\n                        \"Male\", \"Female\", \"Male\", \"Male\",\n                        \"Female\", \"Male\"],\n              \n             'Age': [20, 21, 19, 18, 25, 26, 32, 41, 20, 19],\n              \n             'Annual Income(k$)': [10, 20, 30, 10, 25, 60, 70,\n                                   15, 21, 22],\n              \n             'Spending Score': [30, 50, 48, 84, 90, 65, 32, 46,\n                                12, 56]}\n\n\n# Create DataFrame\ndata = pd.DataFrame(data_dict)\n\n\n# Write to CSV file\ndata.to_csv(\"Customers.csv\")\n\n\n# Print the output.\nprint(data)"
        ],
        [
         "42",
         "Write a Pandas program to rename names of columns and specific labels of the Main Index of the MultiIndex dataframe. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd \rimport numpy as np\rsales_arrays = [['sale1', 'sale1', 'sale2', 'sale2', 'sale3', 'sale3', 'sale4', 'sale4'],\r          ['city1', 'city2', 'city1', 'city2', 'city1', 'city2', 'city1', 'city2']]\rsales_tuples = list(zip(*sales_arrays))\rsales_index = pd.MultiIndex.from_tuples(sales_tuples, names=['sale', 'city'])\rprint(sales_tuples)\rprint(\"\\nConstruct a Dataframe using the said MultiIndex levels: \")\rdf = pd.DataFrame(np.random.randn(8, 5), index=sales_index)\rprint(df)\rprint(\"\\nRename the columns name of the said dataframe\")\rdf1 = df.rename(columns={0: \"col1\", 1: \"col2\", 2:\"col3\", 3:\"col4\", 4:\"col5\"})\rprint(df1)\rprint(\"\\nRename specific labels of the main index of the DataFrame\")\rdf2 = df1.rename(index={\"sale2\": \"S2\", \"city2\": \"C2\"})\rprint(df2)\r"
        ],
        [
         "43",
         "Write a Pandas program to check whether only proper case or title case is present in a given column of a DataFrame. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rdf = pd.DataFrame({\r    'company_code': ['Abcd','EFGF', 'Hhhh', 'abcd', 'EAWQaaa'],\r    'date_of_sale ': ['12/05/2002','16/02/1999','25/09/1998','12/02/2022','15/09/1997'],\r    'sale_amount': [12348.5, 233331.2, 22.5, 2566552.0, 23.0]})\r\rprint(\"Original DataFrame:\")\rprint(df)\rprint(\"\\nIs proper case or title case?\")\rdf['company_code_is_title'] = list(map(lambda x: x.istitle(), df['company_code']))\rprint(df)\r"
        ],
        [
         "44",
         "Write a Pandas program to find which years have all non-zero values and which years have any non-zero values from world alcohol consumption dataset. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd \r\r# World alcohol consumption data\rw_a_con = pd.read_csv('world_alcohol.csv')\rprint(\"World alcohol consumption sample data:\")\rprint(w_a_con.head())\rprint(\"\\nFind which years have all non-zero values:\")\rprint(w_a_con.loc[:,w_a_con.all()])\rprint(\"\\nFind which years have any non-zero values:\")\rprint(w_a_con.loc[:,w_a_con.any()])\r"
        ],
        [
         "45",
         "Write a Pandas program to create a combination from two dataframes where a column id combination appears more than once in both dataframes.",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rdata1 = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],\r                     'key2': ['K0', 'K1', 'K0', 'K1'],\r                     'P': ['P0', 'P1', 'P2', 'P3'],\r                     'Q': ['Q0', 'Q1', 'Q2', 'Q3']}) \rdata2 = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],\r                      'key2': ['K0', 'K0', 'K0', 'K0'],\r                      'R': ['R0', 'R1', 'R2', 'R3'],\r                      'S': ['S0', 'S1', 'S2', 'S3']})\rprint(\"Original DataFrames:\")\rprint(data1)\rprint(\"--------------------\")\rprint(data2)\rprint(\"\\nMerged Data (many-to-many join case):\")\rresult = pd.merge(data1, data2, on='key1')\rprint(result)\r"
        ],
        [
         "46",
         "Write a Pandas program to create a series of Timestamps from a DataFrame of integer or string columns. Also create a series of Timestamps using specified columns. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rdf = pd.DataFrame({'year': [2018, 2019, 2020],\r                   'month': [2, 3, 4],\r                   'day': [4, 5, 6],\r                   'hour': [2, 3, 4]})\rprint(\"Original dataframe:\")\rprint(df)\rresult = pd.to_datetime(df)\rprint(\"\\nSeries of Timestamps from the said dataframe:\")\rprint(result)\rprint(\"\\nSeries of Timestamps using specified columns:\")\rprint(pd.to_datetime(df[['year', 'month', 'day']]))\r"
        ],
        [
         "47",
         "Write a Pandas program to calculate the number of characters in each word in a given series. ",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "import pandas as pd\rseries1 = pd.Series(['Php', 'Python', 'Java', 'C#'])\rprint(\"Original Series:\")\rprint(series1)\rresult = series1.map(lambda x: len(x))\rprint(\"\\nNumber of characters in each word in the said series:\")\rprint(result)\r"
        ],
        [
         "48",
         "Write a Pandas program to import given excel data (employee.xlsx ) into a Pandas dataframe and sort based on multiple given columns. ",
         "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",
         "import pandas as pd\rimport numpy as np\rdf = pd.read_excel('E:\\employee.xlsx')\rresult = df.sort_values(by=['first_name','last_name'],ascending=[0,1])\rresult\r"
        ],
        [
         "49",
         "Split a text column into two columns in Pandas DataFrame in Python",
         "['Province_State', 'Country_Region', 'Last_Update', 'Lat', 'Long_', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'FIPS', 'Incident_Rate', 'Total_Test_Results', 'People_Hospitalized', 'Case_Fatality_Ratio', 'UID', 'ISO3', 'Testing_Rate', 'Hospitalization_Rate', 'Date', 'People_Tested', 'Mortality_Rate']",
         "# import Pandas as pd\nimport pandas as pd\n   \n# create a new data frame\ndf = pd.DataFrame({'Name': ['John Larter', 'Robert Junior', 'Jonny Depp'],\n                   'Age':[32, 34, 36]})\n   \nprint(\"Given Dataframe is :\\n\",df)\n   \n# bydefault splitting is done on the basis of single space.\nprint(\"\\nSplitting 'Name' column into two different columns :\\n\",\n                                  df.Name.str.split(expand=True))"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 974
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>columns</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Write a Pandas program to Combine two DataFram...</td>\n",
       "      <td>['Province_State', 'Country_Region', 'Last_Upd...</td>\n",
       "      <td>import pandas as pd\\rdf1 = pd.DataFrame({'A': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Make a Pandas DataFrame with two-dimensional l...</td>\n",
       "      <td>['Province_State', 'Country_Region', 'Last_Upd...</td>\n",
       "      <td># import pandas as pd \\nimport pandas as pd  \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Write a Pandas program to remove the html tags...</td>\n",
       "      <td>['Province_State', 'Country_Region', 'Last_Upd...</td>\n",
       "      <td>import pandas as pd\\rimport re as re\\rdf = pd....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Write a Pandas program to create a Pivot table...</td>\n",
       "      <td>['Province_State', 'Country_Region', 'Last_Upd...</td>\n",
       "      <td>import pandas as pd\\rimport numpy as np\\rdf = ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apply uppercase to a column in Pandas datafram...</td>\n",
       "      <td>['sepal length (cm)', 'sepal width (cm)', 'pet...</td>\n",
       "      <td># Import pandas package \\nimport pandas as pd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>Write a Pandas program to get all the sighting...</td>\n",
       "      <td>['Province_State', 'Country_Region', 'Last_Upd...</td>\n",
       "      <td>import pandas as pd\\rdf = pd.read_csv(r'ufo.cs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>Write a Pandas program to extract words starti...</td>\n",
       "      <td>['Province_State', 'Country_Region', 'Last_Upd...</td>\n",
       "      <td>import pandas as pd\\rimport re as re\\rdf = pd....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Convert multiple JSON files to CSV Python</td>\n",
       "      <td>['sepal length (cm)', 'sepal width (cm)', 'pet...</td>\n",
       "      <td># importing packages\\nimport pandas as pd\\n  \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>Write a Pandas program to filter all records s...</td>\n",
       "      <td>['sepal length (cm)', 'sepal width (cm)', 'pet...</td>\n",
       "      <td>import pandas as pd\\r# World alcohol consumpti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>Creating a Pandas Series from Lists in Python</td>\n",
       "      <td>['Province_State', 'Country_Region', 'Last_Upd...</td>\n",
       "      <td># import pandas as pd\\nimport pandas as pd\\n  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>974 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     x  \\\n",
       "0    Write a Pandas program to Combine two DataFram...   \n",
       "1    Make a Pandas DataFrame with two-dimensional l...   \n",
       "2    Write a Pandas program to remove the html tags...   \n",
       "3    Write a Pandas program to create a Pivot table...   \n",
       "4    Apply uppercase to a column in Pandas datafram...   \n",
       "..                                                 ...   \n",
       "969  Write a Pandas program to get all the sighting...   \n",
       "970  Write a Pandas program to extract words starti...   \n",
       "971          Convert multiple JSON files to CSV Python   \n",
       "972  Write a Pandas program to filter all records s...   \n",
       "973      Creating a Pandas Series from Lists in Python   \n",
       "\n",
       "                                               columns  \\\n",
       "0    ['Province_State', 'Country_Region', 'Last_Upd...   \n",
       "1    ['Province_State', 'Country_Region', 'Last_Upd...   \n",
       "2    ['Province_State', 'Country_Region', 'Last_Upd...   \n",
       "3    ['Province_State', 'Country_Region', 'Last_Upd...   \n",
       "4    ['sepal length (cm)', 'sepal width (cm)', 'pet...   \n",
       "..                                                 ...   \n",
       "969  ['Province_State', 'Country_Region', 'Last_Upd...   \n",
       "970  ['Province_State', 'Country_Region', 'Last_Upd...   \n",
       "971  ['sepal length (cm)', 'sepal width (cm)', 'pet...   \n",
       "972  ['sepal length (cm)', 'sepal width (cm)', 'pet...   \n",
       "973  ['Province_State', 'Country_Region', 'Last_Upd...   \n",
       "\n",
       "                                                     y  \n",
       "0    import pandas as pd\\rdf1 = pd.DataFrame({'A': ...  \n",
       "1    # import pandas as pd \\nimport pandas as pd  \\...  \n",
       "2    import pandas as pd\\rimport re as re\\rdf = pd....  \n",
       "3    import pandas as pd\\rimport numpy as np\\rdf = ...  \n",
       "4    # Import pandas package \\nimport pandas as pd ...  \n",
       "..                                                 ...  \n",
       "969  import pandas as pd\\rdf = pd.read_csv(r'ufo.cs...  \n",
       "970  import pandas as pd\\rimport re as re\\rdf = pd....  \n",
       "971  # importing packages\\nimport pandas as pd\\n  \\...  \n",
       "972  import pandas as pd\\r# World alcohol consumpti...  \n",
       "973  # import pandas as pd\\nimport pandas as pd\\n  ...  \n",
       "\n",
       "[974 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Download latest version\n",
    "# path = kagglehub.dataset_download(\"linkanjarad/coding-problems-and-solution-python-code\")\n",
    "\n",
    "# file_path = os.path.join(path, 'ProblemSolutionPythonV3.csv')\n",
    "# df = pd.read_csv(file_path)\n",
    "# df.drop(df.columns[0], axis=1, inplace=True)\n",
    "# df = df.loc[df['Python Code'].str.contains('pandas', case=False, na=False)]\n",
    "# df\n",
    "\n",
    "path = '/mnt/c/Users/Pavilion/Documents/BYU-Idaho/Classwork/Winter 2025/DS499/training_data.csv'\n",
    "df = pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 13:52:06.475760: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741809126.500352     907 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741809126.505746     907 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-12 13:52:06.542952: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CodeGenForCausalLM(\n",
       "  (transformer): CodeGenModel(\n",
       "    (wte): Embedding(51200, 1024)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-19): 20 x CodeGenBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CodeGenAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (mlp): CodeGenMLP(\n",
       "          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading in pre-trained model: CodeLlama (https://huggingface.co/Salesforce/codegen-350M-mono)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Tell PyTorch which GPU to use\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable tokenizer parallelism to prevent crashes\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' # Set CUDA_LAUNCH_BLOCKING for debugging\n",
    "\n",
    "pretrained = \"Salesforce/codegen-350M-mono\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained)\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained)\n",
    "\n",
    "# Model pruning\n",
    "\n",
    "def apply_magnitude_pruning(model, amount=0.2):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')  # Optionally, remove the pruned weights\n",
    "\n",
    "apply_magnitude_pruning(model, .25)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define column names, user queries, and columns in query\n",
    "column_names = df['columns'].explode().unique()  # All unique column names across all rows\n",
    "user_queries = df['x']  # Column with the user's query\n",
    "columns_in_query = df['columns']  # Each row has a list of columns used in the query\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def equalize_and_concatenate(tensor1, tensor2):\n",
    "    batch_size, seq_len, feat_dim = tensor1.shape\n",
    "\n",
    "    expanded_tensor2 = tensor2[:seq_len, :].unsqueeze(0).expand(batch_size, -1, -1)\n",
    "    combined_tensor = torch.cat((tensor1, expanded_tensor2), dim=-1)\n",
    "\n",
    "    # print(f'Combined dimensions: {combined_tensor.shape}')\n",
    "    \n",
    "    # Concatenate along the last dimension (feature dimension)\n",
    "    return combined_tensor\n",
    "\n",
    "\n",
    "# Define the ColumnNameEmbedder class\n",
    "class ColumnNameEmbedder(nn.Module):\n",
    "    def __init__(self, column_names, embedding_dim=16):\n",
    "        super(ColumnNameEmbedder, self).__init__()\n",
    "        self.column_name_to_idx = {name: idx for idx, name in enumerate(column_names)}\n",
    "        self.embedding = nn.Embedding(len(column_names), embedding_dim)\n",
    "\n",
    "    def forward(self, columns):\n",
    "        column_indices = [self.column_name_to_idx[col] for col in columns]\n",
    "        column_indices = torch.tensor(column_indices, dtype=torch.long, device=self.embedding.weight.device)\n",
    "        column_embeddings = self.embedding(column_indices)\n",
    "        return column_embeddings\n",
    "\n",
    "# Define the EntityQueryModel class\n",
    "class EntityQueryModel(nn.Module):\n",
    "    def __init__(self, column_embedder, hidden_dim=128, embedding_dim=16):\n",
    "        super(EntityQueryModel, self).__init__()\n",
    "        self.column_embedder = column_embedder\n",
    "        self.embedding_dim = embedding_dim  # Set embedding_dim as an attribute\n",
    "\n",
    "        # Load the tokenizer and model for query embeddings\n",
    "        self.query_tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "        self.query_encoder = AutoModel.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "        self.query_tokenizer.pad_token = self.query_tokenizer.eos_token\n",
    "\n",
    "        # LSTM and output layer\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim * 2, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, self.query_tokenizer.vocab_size)\n",
    "\n",
    "        # Projection layer to match embedding sizes if needed\n",
    "        self.query_projection = nn.Linear(self.query_encoder.config.hidden_size, embedding_dim)\n",
    "        self.column_projection = nn.Linear(embedding_dim, embedding_dim)  # Use the passed embedding_dim\n",
    "\n",
    "    def forward(self, query, columns):\n",
    "        device = self.lstm.weight_ih_l0.device\n",
    "\n",
    "        # Tokenize and encode the query\n",
    "        query_tokens = self.query_tokenizer(query, padding=True, truncation=False, return_tensors=\"pt\").to(device)\n",
    "        query_embedding = self.query_encoder(**query_tokens).last_hidden_state  # Shape: (batch, seq_len, hidden_dim)\n",
    "        query_embedding = self.query_projection(query_embedding)  # Project to (batch, seq_len, embedding_dim)\n",
    "\n",
    "        # Get column embeddings\n",
    "        column_embeddings = self.column_embedder(columns).to(device)  # Assuming this outputs (batch, seq_len, embedding_dim)\n",
    "        column_embeddings = self.column_projection(column_embeddings)  # Ensure correct embedding size\n",
    "\n",
    "        # Equalize and concatenate embeddings\n",
    "        combined_embeddings = equalize_and_concatenate(query_embedding, column_embeddings)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        lstm_output, _ = self.lstm(combined_embeddings)\n",
    "        output = self.fc(lstm_output)  # Use last LSTM output\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Define the tokenize_inputs function\n",
    "def tokenize_inputs(values):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized = tokenizer(values.astype(str).tolist(), padding='max_length', max_length=1000, truncation=False, return_tensors=\"pt\")\n",
    "    return tokenized\n",
    "\n",
    "# Prepare data for training\n",
    "batch_size = 2\n",
    "X = df['x']\n",
    "y = df['y']\n",
    "\n",
    "train_inputs, val_inputs, train_outputs, val_outputs = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_inputs_tokenized = tokenize_inputs(train_inputs)\n",
    "val_inputs_tokenized = tokenize_inputs(val_inputs)\n",
    "train_outputs_tokenized = tokenize_inputs(train_outputs)\n",
    "val_outputs_tokenized = tokenize_inputs(val_outputs)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_inputs_tokenized['input_ids'],\n",
    "    'attention_mask': train_inputs_tokenized['attention_mask'],\n",
    "    'labels': train_outputs_tokenized['input_ids'],\n",
    "}).with_format(type='torch')\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'input_ids': val_inputs_tokenized['input_ids'],\n",
    "    'attention_mask': val_inputs_tokenized['attention_mask'],\n",
    "    'labels': val_outputs_tokenized['input_ids'],\n",
    "}).with_format(type='torch')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExpected Output:\\n\\nCombined dimensions: torch.Size([4, 28, 32])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding = torch.randn(4, 28, 16)  # [batch_size, seq_len, query_dim]\n",
    "column_embeddings = torch.randn(974, 16)  # [num_columns, column_dim]\n",
    "\n",
    "combined_embeddings = equalize_and_concatenate(query_embedding, column_embeddings)\n",
    "'''\n",
    "Expected Output:\n",
    "\n",
    "Combined dimensions: torch.Size([4, 28, 32])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 in progress...\n",
      "Epoch [1/12], Loss: 10.447545584654197, Accuracy: 0.008035970534774705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:94: operator(): block: [6578,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Filter out invalid labels\u001b[39;00m\n\u001b[1;32m    112\u001b[0m valid_mask \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;241m<\u001b[39m outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mas_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    114\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels[valid_mask]\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Debugging: Check for invalid labels\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the column name embedder\n",
    "column_name_embedder = ColumnNameEmbedder(column_names)\n",
    "\n",
    "# Instantiate the entity query model\n",
    "entity_query_model = EntityQueryModel(column_embedder=column_name_embedder)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)  # Cross entropy loss function\n",
    "optimizer = optim.Adam(entity_query_model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 12  # Adjust the number of epochs as needed\n",
    "patience = 3  # Number of epochs to wait for improvement before stopping\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "entity_query_model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs-1):\n",
    "    print(f'Epoch {epoch + 1} in progress...')\n",
    "    entity_query_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch['input_ids'].to(device)  # Encoded user query\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)  # Encoded Python code (target sequence)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Generate logits for next token prediction\n",
    "        inputs_list = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "        outputs = entity_query_model(inputs_list, columns_in_query)\n",
    "\n",
    "        if epoch == 0 and batch == 0:\n",
    "            print(f'inputs_list:\\n{inputs_list}')\n",
    "            print(f\"Outputs before flattening: {outputs.shape}\")  # Should be (batch_size, seq_len, vocab_size)\n",
    "            print(f\"Labels before flattening: {labels.shape}\")    # Should be (batch_size, seq_len)\n",
    "\n",
    "            # Padding outputs to a length of 1000 tokens (truncating if necessary)\n",
    "        max_length = 1000\n",
    "        seq_len = outputs.size(1)\n",
    "\n",
    "        if seq_len < max_length:\n",
    "            # Pad the outputs to match the required max length\n",
    "            padding = (0, 0, 0, max_length - seq_len)  # (pad_left, pad_right, pad_top, pad_bottom)\n",
    "            outputs = F.pad(outputs, padding, value=tokenizer.pad_token_id)\n",
    "        elif seq_len > max_length:\n",
    "            # Truncate if the sequence length exceeds the max_length\n",
    "            outputs = outputs[:, :max_length, :]\n",
    "\n",
    "        outputs = outputs.view(-1, outputs.size(-1))  # Flatten to (batch_size * seq_len, vocab_size)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Filter out invalid labels\n",
    "        valid_mask = labels < outputs.size(-1)\n",
    "        outputs = outputs[valid_mask]\n",
    "        labels = labels[valid_mask]\n",
    "\n",
    "        if torch.any(labels >= outputs.size(-1)):\n",
    "            print(f\"Invalid label found: {labels[labels >= outputs.size(-1)]}\")\n",
    "            raise ValueError(\"Invalid label found\")\n",
    "\n",
    "        assert outputs.shape[0] == labels.shape[0], f\"Mismatch: Outputs shape {outputs.shape}, Labels shape {labels.shape}\"\n",
    "\n",
    "        # Calculate Loss\n",
    "        if epoch == 0 and batch == 0:\n",
    "            print(f\"Inputs shape: {inputs.shape}, Outputs shape: {outputs.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted_tokens = outputs.argmax(dim=-1)  # Get most likely token per position\n",
    "        mask = labels != tokenizer.pad_token_id  # Ignore padding tokens\n",
    "\n",
    "        correct_predictions += (predicted_tokens[mask] == labels[mask]).sum().item()\n",
    "        total_tokens += mask.sum().item()  # Count only valid tokens\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    epoch_accuracy = correct_predictions / total_tokens\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}, Accuracy: {epoch_accuracy}\")\n",
    "\n",
    "    # Validation loop\n",
    "    entity_query_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            inputs_list = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = entity_query_model(inputs_list, columns_in_query)\n",
    "            \n",
    "            # Compute the loss\n",
    "            outputs = outputs.view(-1, outputs.size(-1))  # Flatten to (batch_size * seq_len, vocab_size)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            # Filter out invalid labels\n",
    "            valid_mask = labels < outputs.size(-1)\n",
    "            outputs = outputs[valid_mask.nonzero(as_tuple=True)]\n",
    "            labels = labels[valid_mask]\n",
    "\n",
    "            # Debugging: Check for invalid labels\n",
    "            if torch.any(labels >= outputs.size(-1)):\n",
    "                print(f\"Invalid label found: {labels[labels >= outputs.size(-1)]}\")\n",
    "                raise ValueError(\"Invalid label found\")\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted_tokens = outputs.argmax(dim=-1)\n",
    "            mask = labels != tokenizer.pad_token_id  # Ignore padding tokens\n",
    "\n",
    "            correct_predictions += (predicted_tokens[mask] == labels[mask]).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "    \n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_accuracy = correct_predictions / total_tokens\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        # Save the best model\n",
    "        torch.save(entity_query_model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
