{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Download latest version\n",
    "# path = kagglehub.dataset_download(\"linkanjarad/coding-problems-and-solution-python-code\")\n",
    "\n",
    "# file_path = os.path.join(path, 'ProblemSolutionPythonV3.csv')\n",
    "# df = pd.read_csv(file_path)\n",
    "# df.drop(df.columns[0], axis=1, inplace=True)\n",
    "# df = df.loc[df['Python Code'].str.contains('pandas', case=False, na=False)]\n",
    "# df\n",
    "\n",
    "path = '/mnt/c/Users/Pavilion/Documents/BYU-Idaho/Classwork/Winter 2025/DS499/training_data.csv'\n",
    "df = pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in pre-trained model: CodeLlama (https://huggingface.co/Salesforce/codegen-350M-mono)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Tell PyTorch which GPU to use\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable tokenizer parallelism to prevent crashes\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' # Set CUDA_LAUNCH_BLOCKING for debugging\n",
    "\n",
    "pretrained = \"Salesforce/codegen-350M-mono\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained)\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained)\n",
    "\n",
    "# Model pruning\n",
    "\n",
    "def apply_magnitude_pruning(model, amount=0.2):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')  # Optionally, remove the pruned weights\n",
    "\n",
    "apply_magnitude_pruning(model, .25)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define column names, user queries, and columns in query\n",
    "column_names = df['columns'].explode().unique()  # All unique column names across all rows\n",
    "user_queries = df['x']  # Column with the user's query\n",
    "columns_in_query = df['columns']  # Each row has a list of columns used in the query\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def equalize_and_concatenate(tensor1, tensor2):\n",
    "    batch_size, seq_len, feat_dim = tensor1.shape\n",
    "\n",
    "    expanded_tensor2 = tensor2[:seq_len, :].unsqueeze(0).expand(batch_size, -1, -1)\n",
    "    combined_tensor = torch.cat((tensor1, expanded_tensor2), dim=-1)\n",
    "\n",
    "    # print(f'Combined dimensions: {combined_tensor.shape}')\n",
    "    \n",
    "    # Concatenate along the last dimension (feature dimension)\n",
    "    return combined_tensor\n",
    "\n",
    "\n",
    "# Define the ColumnNameEmbedder class\n",
    "class ColumnNameEmbedder(nn.Module):\n",
    "    def __init__(self, column_names, embedding_dim=16):\n",
    "        super(ColumnNameEmbedder, self).__init__()\n",
    "        self.column_name_to_idx = {name: idx for idx, name in enumerate(column_names)}\n",
    "        self.embedding = nn.Embedding(len(column_names), embedding_dim)\n",
    "\n",
    "    def forward(self, columns):\n",
    "        column_indices = [self.column_name_to_idx[col] for col in columns]\n",
    "        column_indices = torch.tensor(column_indices, dtype=torch.long, device=self.embedding.weight.device)\n",
    "        column_embeddings = self.embedding(column_indices)\n",
    "        return column_embeddings\n",
    "\n",
    "# Define the EntityQueryModel class\n",
    "class EntityQueryModel(nn.Module):\n",
    "    def __init__(self, column_embedder, hidden_dim=128, embedding_dim=16):\n",
    "        super(EntityQueryModel, self).__init__()\n",
    "        self.column_embedder = column_embedder\n",
    "        self.embedding_dim = embedding_dim  # Set embedding_dim as an attribute\n",
    "\n",
    "        # Load the tokenizer and model for query embeddings\n",
    "        self.query_tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "        self.query_encoder = AutoModel.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "        self.query_tokenizer.pad_token = self.query_tokenizer.eos_token\n",
    "\n",
    "        # LSTM and output layer\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim * 2, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, self.query_tokenizer.vocab_size)\n",
    "\n",
    "        # Projection layer to match embedding sizes if needed\n",
    "        self.query_projection = nn.Linear(self.query_encoder.config.hidden_size, embedding_dim)\n",
    "        self.column_projection = nn.Linear(embedding_dim, embedding_dim)  # Use the passed embedding_dim\n",
    "\n",
    "    def forward(self, query, columns):\n",
    "        device = self.lstm.weight_ih_l0.device\n",
    "\n",
    "        # Tokenize and encode the query\n",
    "        query_tokens = self.query_tokenizer(query, padding=True, truncation=False, return_tensors=\"pt\").to(device)\n",
    "        query_embedding = self.query_encoder(**query_tokens).last_hidden_state  # Shape: (batch, seq_len, hidden_dim)\n",
    "        query_embedding = self.query_projection(query_embedding)  # Project to (batch, seq_len, embedding_dim)\n",
    "\n",
    "        # Get column embeddings\n",
    "        column_embeddings = self.column_embedder(columns).to(device)  # Assuming this outputs (batch, seq_len, embedding_dim)\n",
    "        column_embeddings = self.column_projection(column_embeddings)  # Ensure correct embedding size\n",
    "\n",
    "        # Equalize and concatenate embeddings\n",
    "        combined_embeddings = equalize_and_concatenate(query_embedding, column_embeddings)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        lstm_output, _ = self.lstm(combined_embeddings)\n",
    "        output = self.fc(lstm_output)  # Use last LSTM output\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Define the tokenize_inputs function\n",
    "def tokenize_inputs(values):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized = tokenizer(values.astype(str).tolist(), padding='max_length', max_length=1000, truncation=False, return_tensors=\"pt\")\n",
    "    return tokenized\n",
    "\n",
    "# Prepare data for training\n",
    "batch_size = 2\n",
    "X = df['x']\n",
    "y = df['y']\n",
    "\n",
    "train_inputs, val_inputs, train_outputs, val_outputs = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_inputs_tokenized = tokenize_inputs(train_inputs)\n",
    "val_inputs_tokenized = tokenize_inputs(val_inputs)\n",
    "train_outputs_tokenized = tokenize_inputs(train_outputs)\n",
    "val_outputs_tokenized = tokenize_inputs(val_outputs)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_inputs_tokenized['input_ids'],\n",
    "    'attention_mask': train_inputs_tokenized['attention_mask'],\n",
    "    'labels': train_outputs_tokenized['input_ids'],\n",
    "}).with_format(type='torch')\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'input_ids': val_inputs_tokenized['input_ids'],\n",
    "    'attention_mask': val_inputs_tokenized['attention_mask'],\n",
    "    'labels': val_outputs_tokenized['input_ids'],\n",
    "}).with_format(type='torch')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = torch.randn(4, 28, 16)  # [batch_size, seq_len, query_dim]\n",
    "column_embeddings = torch.randn(974, 16)  # [num_columns, column_dim]\n",
    "\n",
    "combined_embeddings = equalize_and_concatenate(query_embedding, column_embeddings)\n",
    "'''\n",
    "Expected Output:\n",
    "\n",
    "Combined dimensions: torch.Size([4, 28, 32])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the column name embedder\n",
    "column_name_embedder = ColumnNameEmbedder(column_names)\n",
    "\n",
    "# Instantiate the entity query model\n",
    "entity_query_model = EntityQueryModel(column_embedder=column_name_embedder)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)  # Cross entropy loss function\n",
    "optimizer = optim.Adam(entity_query_model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 12  # Adjust the number of epochs as needed\n",
    "patience = 3  # Number of epochs to wait for improvement before stopping\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "entity_query_model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs-1):\n",
    "    print(f'Epoch {epoch + 1} in progress...')\n",
    "    entity_query_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch['input_ids'].to(device)  # Encoded user query\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)  # Encoded Python code (target sequence)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Generate logits for next token prediction\n",
    "        inputs_list = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "        outputs = entity_query_model(inputs_list, columns_in_query)\n",
    "\n",
    "        if epoch == 0 and batch == 0:\n",
    "            print(f'inputs_list:\\n{inputs_list}')\n",
    "            print(f\"Outputs before flattening: {outputs.shape}\")  # Should be (batch_size, seq_len, vocab_size)\n",
    "            print(f\"Labels before flattening: {labels.shape}\")    # Should be (batch_size, seq_len)\n",
    "\n",
    "            # Padding outputs to a length of 1000 tokens (truncating if necessary)\n",
    "        max_length = 1000\n",
    "        seq_len = outputs.size(1)\n",
    "\n",
    "        if seq_len < max_length:\n",
    "            # Pad the outputs to match the required max length\n",
    "            padding = (0, 0, 0, max_length - seq_len)  # (pad_left, pad_right, pad_top, pad_bottom)\n",
    "            outputs = F.pad(outputs, padding, value=tokenizer.pad_token_id)\n",
    "        elif seq_len > max_length:\n",
    "            # Truncate if the sequence length exceeds the max_length\n",
    "            outputs = outputs[:, :max_length, :]\n",
    "\n",
    "        outputs = outputs.view(-1, outputs.size(-1))  # Flatten to (batch_size * seq_len, vocab_size)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Filter out invalid labels\n",
    "        valid_mask = labels < outputs.size(-1)\n",
    "        outputs = outputs[valid_mask]\n",
    "        labels = labels[valid_mask]\n",
    "\n",
    "        if torch.any(labels >= outputs.size(-1)):\n",
    "            print(f\"Invalid label found: {labels[labels >= outputs.size(-1)]}\")\n",
    "            raise ValueError(\"Invalid label found\")\n",
    "\n",
    "        assert outputs.shape[0] == labels.shape[0], f\"Mismatch: Outputs shape {outputs.shape}, Labels shape {labels.shape}\"\n",
    "\n",
    "        # Calculate Loss\n",
    "        if epoch == 0 and batch == 0:\n",
    "            print(f\"Inputs shape: {inputs.shape}, Outputs shape: {outputs.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted_tokens = outputs.argmax(dim=-1)  # Get most likely token per position\n",
    "        mask = labels != tokenizer.pad_token_id  # Ignore padding tokens\n",
    "\n",
    "        correct_predictions += (predicted_tokens[mask] == labels[mask]).sum().item()\n",
    "        total_tokens += mask.sum().item()  # Count only valid tokens\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    epoch_accuracy = correct_predictions / total_tokens\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}, Accuracy: {epoch_accuracy}\")\n",
    "\n",
    "    # Validation loop\n",
    "    entity_query_model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            inputs_list = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = entity_query_model(inputs_list, columns_in_query)\n",
    "            \n",
    "            # Compute the loss\n",
    "            outputs = outputs.view(-1, outputs.size(-1))  # Flatten to (batch_size * seq_len, vocab_size)\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            # Filter out invalid labels\n",
    "            valid_mask = labels < outputs.size(-1)\n",
    "            outputs = outputs[valid_mask.nonzero(as_tuple=True)]\n",
    "            labels = labels[valid_mask]\n",
    "\n",
    "            # Debugging: Check for invalid labels\n",
    "            if torch.any(labels >= outputs.size(-1)):\n",
    "                print(f\"Invalid label found: {labels[labels >= outputs.size(-1)]}\")\n",
    "                raise ValueError(\"Invalid label found\")\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted_tokens = outputs.argmax(dim=-1)\n",
    "            mask = labels != tokenizer.pad_token_id  # Ignore padding tokens\n",
    "\n",
    "            correct_predictions += (predicted_tokens[mask] == labels[mask]).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "    \n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_accuracy = correct_predictions / total_tokens\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        # Save the best model\n",
    "        torch.save(entity_query_model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
